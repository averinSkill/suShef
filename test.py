from llama_cpp import Llama
from datetime import datetime


t0 = datetime.now()
text = """
            всем огромный привет Сегодня готовлю сырники методом проб и ошибок я вывела для себя рецепт идеальных сырников других рецептов теперь для меня не существует такие сырники просто идеальные при жарке не расплываются остаются пышными И самое главное у них тот самый творожный вкус за который их любят Следуйте моему рецепту и у вас всё получится берём творог и я выбираю творог мягкий Если будете брать творог жестковатый то лучше его пробить блендером или протереть через сито самое главное условие идеальных сырников - это сухой творог поэтому обязательно творог Сначала отжимаем я кладу в марлю и отжимания насколько это возможно разбиваем одно яйцо Добавляем сахар много сахара добавлять Не рекомендую творога Не забудьте нажать на колокольчик чтобы не пропустить новые видео Желаю вам приятного аппетита вкусно Будьте здоровы
      """
# text = 'всем огромный привет Сегодня готовлю сырники методом проб и ошибок я вывела для себя рецепт идеальных сырников других рецептов теперь для меня не существует такие сырники просто идеальные при жарке не расплываются остаются пышными И самое главное у них тот самый творожный вкус за который их любят Следуйте моему рецепту и у вас всё получится берём творог и я выбираю творог мягкий Если будете брать творог жестковатый то лучше его пробить блендером или протереть через сито самое главное условие идеальных сырников - это сухой творог поэтому обязательно творог Сначала отжимаем я кладу в марлю и отжимания насколько это возможно разбиваем одно яйцо Добавляем сахар много сахара добавлять Не рекомендую творога Не забудьте нажать на колокольчик чтобы не пропустить новые видео Желаю вам приятного аппетита вкусно Будьте здоровы'
llm = Llama(
        # model_path = "llama-2-7b-chat.Q4_K_M.gguf",# долгая, но тоже говно
      model_path="./DeepSeek-R1-Distill-Qwen-1.5B-f16.gguf", # лучшая на сегодня
        # model_path="./DeepSeek-R1-Distill-Qwen-1.5B-Q6_K.gguf", #быстрая, но полное говно
      n_gpu_layers=-1, # Uncomment to use GPU acceleration
      # seed=1337, # Uncomment to set a specific seed
      # n_ctx=2048, # Uncomment to increase the context window
)
output = llm(
        # f"Q: Сделай суммаризацию текста? {text} A: ",
        # f"Выдели ключевые слова {text}", # Prompt
        # # f"О чем текст? {text}", # Prompt
      f"Q: В тексте есть рецепт приготовления блюда? {text} A: ", # Prompt
      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window
      stop=["Q:", "\n"], # Stop generating just before the model would generate a new question
      echo=True # Echo the prompt back in the output
) # Generate a completion, can also call create_completion

print(output)
print(output['choices'][0]["text"].split("A: \t")[-1])

# llm_recipe(text)
print(datetime.now() - t0)

